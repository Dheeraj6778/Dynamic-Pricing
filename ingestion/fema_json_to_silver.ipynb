{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74c4e8d-ba56-4093-bf07-6641baa5896f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, input_file_name, current_timestamp, lit, upper, trim, concat, year, month, monthname, datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd1db7e-6b05-40c7-8a82-f3d592d2fe56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reading all run_metadata.json and figuring out the latest run_id and reading only that run\n",
    "\n",
    "runs = dbutils.fs.ls(\"/Volumes/climate-risk/bronze/fema_raw\")\n",
    "base_path = \"/Volumes/climate-risk/bronze/fema_raw/\"\n",
    "for run in runs:\n",
    "    meta_data_file = f'{base_path}/{run.name}/metadata/run_metadata.json'\n",
    "    metadata = spark.read.option(\"multiline\",\"true\").json(meta_data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417c7d82-f1e6-4cc1-b39d-66df5e37db0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "meta_df = spark.read.option(\"multiline\",\"true\")\\\n",
    "    .json(f'{base_path}*/metadata/run_metadata.json')\n",
    "latest_run = meta_df.filter(\"status ='SUCCESS'\")\\\n",
    "    .orderBy(\"ingest_end_ts\",ascending=False)\\\n",
    "    .limit(1)\n",
    "\n",
    "latest_run_id = latest_run.select(\"run_id\").collect()[0]['run_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe00e77-bb3b-4d37-abbb-35053fb173d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_data = spark.read.json(f\"{base_path}run_id={latest_run_id}/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45084c7-977f-4cef-9940-bb639a220303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    bronze_data\n",
    "    .select(explode(\"DisasterDeclarationsSummaries\").alias(\"d\"))\n",
    "    .select(\"d.*\", \"_metadata.file_path\")\n",
    "    .withColumnRenamed(\"_metadata.file_path\", \"source_file\")\n",
    "    .withColumn(\"ingest_ts\", current_timestamp())\n",
    "    .withColumn(\"run_id\", lit(latest_run_id))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f5216b-a5c9-483b-9744-9b84f4c9e013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "date_cols = [\"declarationDate\", \"incidentBeginDate\", \"incidentEndDate\"]\n",
    "for date_col in date_cols:\n",
    "    df = df.withColumn(\n",
    "        date_col,\n",
    "        col(date_col).cast(\"date\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bdacfb-187b-4a84-a756-2f3f5b3246db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"fips_code\", concat(col(\"fipsStateCode\"),col(\"fipsCountyCode\")))\n",
    "df = df.withColumn(\"state\", upper(col(\"state\")))\n",
    "trim_cols = [\"designatedArea\", \"incidentType\",\"declarationType\"]\n",
    "for trim_col in trim_cols:\n",
    "    df = df.withColumn(trim_col, trim(col(trim_col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b37610a-52e6-4648-82a2-663058718d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"record_id\",concat(col(\"disasterNumber\"),col(\"state\"),col(\"designatedArea\"),col(\"fips_code\")))\n",
    "#drop duplicates based on record_id\n",
    "df = df.dropDuplicates(['record_id'])\n",
    "\n",
    "df = df.withColumn(\"declarationYear\",year(col(\"declarationDate\")))\\\n",
    "    .withColumn(\"declarationMonth\",monthname(col(\"declarationDate\")))\\\n",
    "    .withColumn(\"incident_duration\",datediff(col(\"incidentEndDate\"),col(\"incidentBeginDate\")))\n",
    "df = df.filter(df.disasterNumber.isNotNull() | df.declarationDate.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c325f3-5815-4470-9f70-6e3148e1f948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_mode = \"\"\n",
    "\n",
    "if spark.catalog.tableExists(\"`climate-risk`.silver.fema_disaster_declarations\"):\n",
    "    write_mode = \"append\"\n",
    "else:\n",
    "    write_mode = \"overwrite\"\n",
    "\n",
    "df.write.format(\"delta\")\\\n",
    "    .mode(write_mode)\\\n",
    "        .saveAsTable(\"`climate-risk`.silver.fema_disaster_declarations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39bb620b-6136-4ea1-9ce2-e186f73e32c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fema_json_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
